{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "descending-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Conv2D, MaxPooling1D, Flatten\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tf2crf import ModelWithCRFLoss, CRF\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promotional-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training and testing data\n",
    "data = pd.read_csv('./MSRA/msra_train.csv') # training data\n",
    "test_data = pd.read_csv('./MSRA/msra_test.csv')# testing data\n",
    "\n",
    "training_char = data.Character.unique()\n",
    "testing_char = test_data.Character.unique()\n",
    "illegal_chars = []\n",
    "\n",
    "for char in testing_char:\n",
    "    if char not in training_char:\n",
    "        illegal_chars.append(char)\n",
    "        \n",
    "test_data = test_data[~ test_data.Character.isin(illegal_chars)]\n",
    "\n",
    "# loading radical dictionary (will use chise in the future)\n",
    "df_radicals = pd.read_csv('./MSRA/chise_radical.csv')                            \n",
    "characters = df_radicals['character'].values\n",
    "radicals = df_radicals['radical_info'].values\n",
    "char_rad_dict = {}\n",
    "for i in range (len(characters)):\n",
    "    exec('char_rad_dict[characters[i]] =' +  radicals[i])\n",
    "\n",
    "    \n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(c, t) for c, t in zip(s[\"Character\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "# training sentence\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "# testing sentence\n",
    "getter_te = SentenceGetter(test_data)\n",
    "sentences_te = getter_te.sentences\n",
    "\n",
    "\n",
    "df_char_token = pd.read_csv('./MSRA/msra_char_token.csv')\n",
    "df_radical_token = pd.read_csv('./MSRA/msra_radical_token.csv')\n",
    "df_tag_token = pd.read_csv('./MSRA/msra_tag_token.csv')\n",
    "\n",
    "char2idx = {}; radical2idx = {}; tag2idx = {}\n",
    "for i in range (len(df_char_token)):\n",
    "    char2idx[df_char_token.char[i]] = df_char_token.token[i]\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "for i in range (len(df_radical_token)):\n",
    "    radical2idx[df_radical_token.radical[i]] = df_radical_token.token[i]\n",
    "idx2radical = {i : r for r, i in radical2idx.items()}\n",
    "\n",
    "for i in range (len(df_tag_token)):\n",
    "    tag2idx[df_tag_token.tag[i]] = df_tag_token.token[i]\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "\n",
    "max_len_char = 90 # the length for each sentence (including character padding)\n",
    "max_len_radical = 8 # the length for each character (including radical padding)\n",
    "\n",
    "# character tokenrization\n",
    "def char_token(sentences, char2idx = char2idx, max_len_char = max_len_char):\n",
    "    X_char = [[char2idx[c[0]] for c in s] for s in sentences]\n",
    "    X_char = pad_sequences(maxlen=max_len_char, sequences=X_char, value=char2idx[\"PAD\"], padding='post', truncating='post')\n",
    "    X_char = np.array(X_char)\n",
    "    return X_char\n",
    "\n",
    "X_char = char_token(sentences)\n",
    "X_char_te = char_token(sentences_te)\n",
    "\n",
    "\n",
    "# radical tokenrization\n",
    "def radical_token(sentences, max_len_char = max_len_char, max_len_radical = max_len_radical,\n",
    "                 radical2idx = radical2idx, char_rad_dict = char_rad_dict):\n",
    "    X_radical = []\n",
    "    for sentence in sentences:\n",
    "        sent_seq = []\n",
    "        for i in range(max_len_char):\n",
    "            word_seq = []\n",
    "            for j in range(max_len_radical): \n",
    "                try:\n",
    "                    char = sentence[i][0][j]\n",
    "                    if char in char_rad_dict.keys():     \n",
    "                        radicals = char_rad_dict[char]\n",
    "                        for i in range(len(radicals)):\n",
    "                            if i < max_len_radical - 1:    \n",
    "                                word_seq.append(radical2idx.get(radicals[i]))\n",
    "                    else:\n",
    "                        word_seq.append(radical2idx.get(\"UNK\"))\n",
    "                except:\n",
    "                    if len(word_seq) < max_len_radical:\n",
    "                        word_seq.append(radical2idx.get(\"PAD\"))\n",
    "            sent_seq.append(word_seq)\n",
    "        X_radical.append((sent_seq))\n",
    "    X_radical = np.array((X_radical))\n",
    "    return X_radical\n",
    "\n",
    "X_radical = radical_token(sentences)\n",
    "X_radical_te = radical_token(sentences_te)\n",
    "\n",
    "\n",
    "def tag_token(sentences, max_len_char = max_len_char, tag2idx = tag2idx):\n",
    "    y = [[tag2idx[c[1]] for c in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len_char, sequences=y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n",
    "    return y\n",
    "\n",
    "y = tag_token(sentences)\n",
    "y_te = tag_token(sentences_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc10b1e68d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dim = 300\n",
    "radical_dim = 200\n",
    "\n",
    "n_chars = len(df_char_token)\n",
    "n_radical = len(df_radical_token)\n",
    "n_tags = len(df_tag_token)\n",
    "\n",
    "# input and embeddings for radicals\n",
    "char_in = Input(shape=(max_len_char,), )\n",
    "\n",
    "emb_char = Embedding(input_dim = n_chars, output_dim = char_dim, input_length = max_len_char, mask_zero = True)(char_in)\n",
    "\n",
    "# input and embeddings for radicals\n",
    "radical_in = Input(shape=(max_len_char, max_len_radical,), )\n",
    "\n",
    "emb_radical = TimeDistributed(Embedding(input_dim=n_radical, output_dim=radical_dim,\n",
    "                           input_length=max_len_radical, mask_zero = True))(radical_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "radical_enc = TimeDistributed(Bidirectional(LSTM(units=radical_dim, return_sequences=False,dropout = 0.5, recurrent_dropout=0.5)))(emb_radical)\n",
    "\n",
    "x = concatenate([emb_char, radical_enc])\n",
    "# main LSTM\n",
    "main_lstm = Bidirectional(LSTM(units=char_dim+radical_dim, return_sequences=True,dropout = 0.5, recurrent_dropout=0.5))(x)\n",
    "dense = TimeDistributed(Dense(char_dim+radical_dim, activation = None))(main_lstm)\n",
    "crf = tfa.layers.CRF(n_tags)\n",
    "out = crf(dense)\n",
    "scheme_1_model = Model(inputs =[char_in, radical_in], outputs = out)\n",
    "scheme_1_model = ModelWithCRFLoss(scheme_1_model)\n",
    "scheme_1_model.load_weights('../diss_result/Scheme_1/cp-0181.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "answering-harvard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc10b113f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = len(df_char_token)\n",
    "n_radical = len(df_radical_token)\n",
    "n_tags = len(df_tag_token)\n",
    "\n",
    "char_dim = 300\n",
    "radical_dim = 300\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len_char,), )\n",
    "\n",
    "emb_char = Embedding(input_dim = n_chars, output_dim = char_dim, input_length = max_len_char, mask_zero = True)(char_in)\n",
    "\n",
    "# input and embeddings for radicals\n",
    "radical_in = Input(shape=(max_len_char, max_len_radical,), )\n",
    "\n",
    "\n",
    "emb_radical = TimeDistributed(Embedding(input_dim=n_radical, output_dim=radical_dim,\n",
    "                           input_length=max_len_radical))(radical_in)\n",
    "\n",
    "dropout = Dropout(0.5)(emb_radical)\n",
    "\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size = 3, filters=radical_dim, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
    "\n",
    "maxpool_out = TimeDistributed(MaxPooling1D(max_len_radical))(conv1d_out)\n",
    "\n",
    "radical = TimeDistributed(Flatten())(maxpool_out)\n",
    "\n",
    "\n",
    "x = concatenate([emb_char, radical])\n",
    "# main LSTM\n",
    "main_lstm = Bidirectional(LSTM(units=radical_dim + char_dim, return_sequences=True,dropout = 0.5, recurrent_dropout=0.5))(x)\n",
    "dense = TimeDistributed(Dense(radical_dim + char_dim, activation = None))(main_lstm)\n",
    "crf = tfa.layers.CRF(n_tags)\n",
    "out = crf(dense)\n",
    "scheme_2_model = Model(inputs =[char_in, radical_in], outputs = out)\n",
    "scheme_2_model = ModelWithCRFLoss(scheme_2_model)\n",
    "scheme_2_model.load_weights('../diss_result/Scheme_2/cp-0086.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operating-authority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc10af67dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = len(df_char_token)\n",
    "n_radical = len(df_radical_token)\n",
    "n_tags = len(df_tag_token)\n",
    "char_dim = 300\n",
    "radical_dim = 0\n",
    "\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len_char,), )\n",
    "emb_char = Embedding(input_dim = n_chars, output_dim = char_dim, input_length = max_len_char, mask_zero = True)(char_in)\n",
    "\n",
    "# main LSTM\n",
    "main_lstm = Bidirectional(LSTM(units=char_dim + radical_dim, return_sequences=True,dropout = 0.5, recurrent_dropout=0.5))(emb_char)\n",
    "dense = TimeDistributed(Dense(char_dim + radical_dim, activation = None))(main_lstm)\n",
    "crf = tfa.layers.CRF(n_tags)\n",
    "out = crf(dense)\n",
    "scheme_3_model = Model(inputs =char_in, outputs = out)\n",
    "scheme_3_model = ModelWithCRFLoss(scheme_3_model)\n",
    "\n",
    "\n",
    "scheme_3_model.load_weights('../diss_result/Scheme_3/cp-0169.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "given-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the padding tokens\n",
    "y_te_mask = y_te != 0\n",
    "y_true = y_te[y_te_mask]\n",
    "# recover tokens to tags\n",
    "y_true_recovered = [[idx2tag[token] for token in y_true]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "green-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of Scheme 1:  0.9027613412228797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.93      0.91      0.92      2641\n",
      "         ORG       0.86      0.86      0.86      1197\n",
      "         PER       0.92      0.88      0.90      1293\n",
      "\n",
      "   micro avg       0.91      0.89      0.90      5131\n",
      "   macro avg       0.91      0.89      0.90      5131\n",
      "weighted avg       0.91      0.89      0.90      5131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_result_scheme_1 = scheme_1_model.predict([X_char_te, X_radical_te])[0]\n",
    "y_pred_scheme_1 = y_pred_result_scheme_1[y_te_mask]\n",
    "# recover tokens to tags\n",
    "y_pred_recovered_scheme_1 = [[idx2tag[token] for token in y_pred_scheme_1]]\n",
    "\n",
    "print('F1-score of Scheme 1: ', f1_score(y_true_recovered, y_pred_recovered_scheme_1))\n",
    "print(classification_report(y_true_recovered, y_pred_recovered_scheme_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "concrete-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of Scheme 2:  0.8992448330683624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.93      0.91      0.92      2641\n",
      "         ORG       0.88      0.84      0.86      1197\n",
      "         PER       0.93      0.87      0.90      1293\n",
      "\n",
      "   micro avg       0.92      0.88      0.90      5131\n",
      "   macro avg       0.91      0.87      0.89      5131\n",
      "weighted avg       0.92      0.88      0.90      5131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_result_scheme_2 = scheme_2_model.predict([X_char_te, X_radical_te])[0]\n",
    "y_pred_scheme_2 = y_pred_result_scheme_2[y_te_mask]\n",
    "# recover tokens to tags\n",
    "y_pred_recovered_scheme_2 = [[idx2tag[token] for token in y_pred_scheme_2]]\n",
    "\n",
    "print('F1-score of Scheme 2: ', f1_score(y_true_recovered, y_pred_recovered_scheme_2))\n",
    "print(classification_report(y_true_recovered, y_pred_recovered_scheme_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of Scheme 3:  0.896388395500296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.93      0.90      0.92      2641\n",
      "         ORG       0.86      0.85      0.85      1197\n",
      "         PER       0.91      0.88      0.90      1293\n",
      "\n",
      "   micro avg       0.91      0.89      0.90      5131\n",
      "   macro avg       0.90      0.88      0.89      5131\n",
      "weighted avg       0.91      0.89      0.90      5131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_result_scheme_3 = scheme_3_model.predict(X_char_te)[0]\n",
    "y_pred_scheme_3 = y_pred_result_scheme_3[y_te_mask]\n",
    "# recover tokens to tags\n",
    "y_pred_recovered_scheme_3 = [[idx2tag[token] for token in y_pred_scheme_3]]\n",
    "\n",
    "print('F1-score of Scheme 3: ', f1_score(y_true_recovered, y_pred_recovered_scheme_3))\n",
    "print(classification_report(y_true_recovered, y_pred_recovered_scheme_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identical-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_generator(idx):\n",
    "    # predicted tokens of a specific sentence\n",
    "    scheme_1_tokens = y_pred_result_scheme_1[idx]\n",
    "    scheme_2_tokens = y_pred_result_scheme_2[idx]\n",
    "    scheme_3_tokens = y_pred_result_scheme_3[idx]\n",
    "    # remove padding tokens\n",
    "    true_tokens = y_te[idx]\n",
    "    mask = true_tokens != 0\n",
    "    true_result = true_tokens[mask]\n",
    "    \n",
    "    scheme_1_result = scheme_1_tokens[mask]\n",
    "    scheme_2_result = scheme_2_tokens[mask]\n",
    "    scheme_3_result = scheme_3_tokens[mask]\n",
    "    \n",
    "    chars = X_char_te[idx][mask]\n",
    "    \n",
    "    \n",
    "    # recover tokens\n",
    "    \n",
    "    chars_recovered = [idx2char[token] for token in chars]\n",
    "    tags_recovered = [idx2tag[token] for token in true_result]\n",
    "    \n",
    "    scheme_1_prediction = [idx2tag[token] for token in scheme_1_result]\n",
    "    scheme_2_prediction = [idx2tag[token] for token in scheme_2_result]\n",
    "    scheme_3_prediction = [idx2tag[token] for token in scheme_3_result]\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(data = {'char': chars_recovered, 'tag': tags_recovered, \n",
    "                             'Prediction of Scheme 1': scheme_1_prediction,\n",
    "                             'Prediction of Scheme 2': scheme_2_prediction,\n",
    "                             'Prediction of Scheme 3': scheme_3_prediction})\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "immune-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_instances = [638, 674, 2970, 3001, 722, 799, 2236, 2403, 2412, 2537, 3140]\n",
    "instances_cycle = itertools.cycle(positive_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "absolute-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "祝九三学社第七次全国代表大会圆满成功！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char</th>\n",
       "      <th>tag</th>\n",
       "      <th>Prediction of Scheme 1</th>\n",
       "      <th>Prediction of Scheme 2</th>\n",
       "      <th>Prediction of Scheme 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>祝</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>九</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>三</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>学</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>社</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>第</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>七</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>次</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>全</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>国</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>代</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>表</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>大</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>会</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>圆</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>满</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>成</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>功</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>！</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   char    tag Prediction of Scheme 1 Prediction of Scheme 2  \\\n",
       "0     祝      O                      O                      O   \n",
       "1     九  B-ORG                  B-ORG                  B-ORG   \n",
       "2     三  I-ORG                  I-ORG                  I-ORG   \n",
       "3     学  I-ORG                  I-ORG                  I-ORG   \n",
       "4     社  I-ORG                  I-ORG                  I-ORG   \n",
       "5     第  I-ORG                  I-ORG                  I-ORG   \n",
       "6     七  I-ORG                  I-ORG                  I-ORG   \n",
       "7     次  I-ORG                  I-ORG                  I-ORG   \n",
       "8     全  I-ORG                  I-ORG                  I-ORG   \n",
       "9     国  I-ORG                  I-ORG                  I-ORG   \n",
       "10    代  I-ORG                  I-ORG                  I-ORG   \n",
       "11    表  I-ORG                  I-ORG                  I-ORG   \n",
       "12    大  I-ORG                  I-ORG                  I-ORG   \n",
       "13    会  I-ORG                  I-ORG                  I-ORG   \n",
       "14    圆      O                      O                      O   \n",
       "15    满      O                      O                      O   \n",
       "16    成      O                      O                      O   \n",
       "17    功      O                      O                      O   \n",
       "18    ！      O                      O                      O   \n",
       "\n",
       "   Prediction of Scheme 3  \n",
       "0                   B-ORG  \n",
       "1                   I-ORG  \n",
       "2                   I-ORG  \n",
       "3                   I-ORG  \n",
       "4                   I-ORG  \n",
       "5                   I-ORG  \n",
       "6                   I-ORG  \n",
       "7                   I-ORG  \n",
       "8                   I-ORG  \n",
       "9                   I-ORG  \n",
       "10                  I-ORG  \n",
       "11                  I-ORG  \n",
       "12                  I-ORG  \n",
       "13                  I-ORG  \n",
       "14                      O  \n",
       "15                      O  \n",
       "16                      O  \n",
       "17                      O  \n",
       "18                      O  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = next(instances_cycle)\n",
    "print(idx)\n",
    "original_sentence = ''.join(instance_generator(idx).char.values)\n",
    "print(original_sentence)\n",
    "instance_generator(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "magnetic-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_list = []\n",
    "# for i in range(len(sentences_te)):\n",
    "#     if len(sentences_te[i]) < 30:\n",
    "#         sentence_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_1_predict = scheme_1_model.predict([X_char_te[sentence_list], X_radical_te[sentence_list]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nonprofit-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_2_predict = scheme_2_model.predict([X_char_te[sentence_list], X_radical_te[sentence_list]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "micro-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_3_predict = scheme_3_model.predict(X_char_te[sentence_list])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rotary-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_1_instance = []\n",
    "# for i in range(len(sentence_list)):\n",
    "#     if not np.array_equal(scheme_1_predict[i], scheme_3_predict[i]):\n",
    "#         scheme_1_instance.append(sentence_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "crazy-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_2_instance = []\n",
    "# for i in range(len(sentence_list)):\n",
    "#     if not np.array_equal(scheme_2_predict[i], scheme_3_predict[i]):\n",
    "#         scheme_2_instance.append(sentence_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "provincial-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(scheme_1_instance).intersection(set(scheme_2_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opened-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_te[3288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "invalid-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.expand_dims(X_char_te[3288], axis=0)\n",
    "# b = np.expand_dims(X_radical_te[3288], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "alternative-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array_equal(scheme_2_model.predict([a,b])[0] ,scheme_1_model.predict([a,b])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "signed-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_3_model.predict(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "million-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_2_model.predict([a,b])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "renewable-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheme_1_model.predict([a,b])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "chinese-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag2idx"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
